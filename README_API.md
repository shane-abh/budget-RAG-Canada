# RAG Chat API - FastAPI Implementation

This is a FastAPI conversion of your RAG (Retrieval Augmented Generation) system. It maintains all your original functionality while providing REST API endpoints and streaming chat capabilities.

## ğŸš€ Quick Start

### 1. Install Dependencies

```bash
pip install -r requirements_api.txt
```

### 2. Setup Environment Variables

Make sure your `.env` file contains:
```
PINECONE_API_KEY=your_pinecone_api_key
GOOGLE_API_KEY=your_google_api_key
```

### 3. Run the FastAPI Server

```bash
# Development mode (with auto-reload)
uvicorn app:app --reload --host 0.0.0.0 --port 8000

# Production mode
uvicorn app:app --host 0.0.0.0 --port 8000
```

### 4. Test the API

Open your browser to:
- **Interactive API Docs**: http://localhost:8000/docs
- **Alternative Docs**: http://localhost:8000/redoc
- **Chat Interface**: Open `test_client.html` in your browser

## ğŸ“¡ API Endpoints

### Health Check
```bash
GET http://localhost:8000/
```
Returns API status and version.

### Create New Chat Session
```bash
POST http://localhost:8000/session/new
```
**Response:**
```json
{
  "session_id": "uuid-string"
}
```

### Chat with Streaming (SSE)
```bash
POST http://localhost:8000/chat/stream
Content-Type: application/json

{
  "session_id": "optional-uuid",
  "message": "What is Canada's response to U.S. tariffs?",
  "use_query_expansion": true
}
```

**Stream Events:**
- `session`: Session information
- `original_query`: Your original question
- `rewritten_query`: Optimized version of your query
- `expanded_terms`: Related terms for better search
- `query_variations`: Different ways to search
- `sources`: Relevant document sources
- `status`: Processing status updates
- `answer_start`: Answer generation begins
- `answer_chunk`: Streamed answer text
- `answer_complete`: Answer finished
- `done`: Process complete

### Upload Document
```bash
POST http://localhost:8000/upload
Content-Type: multipart/form-data

file: your_document.pdf
```

**Response:**
```json
{
  "message": "Document uploaded and indexed successfully",
  "document_name": "your_document.pdf",
  "num_pages": 50,
  "num_chunks": 85,
  "processing_time": 45.2
}
```

### List Documents
```bash
GET http://localhost:8000/documents
```

**Response:**
```json
{
  "index_name": "rag-documents",
  "documents": ["document1.pdf", "document2.pdf"],
  "total": 2
}
```

### Get Session History
```bash
GET http://localhost:8000/session/{session_id}/history
```

**Response:**
```json
{
  "session_id": "uuid-string",
  "messages": [
    {
      "role": "user",
      "content": "What is...",
      "timestamp": 1234567890.123,
      "metadata": {}
    }
  ],
  "total_messages": 5
}
```

### Delete Session
```bash
DELETE http://localhost:8000/session/{session_id}
```

## ğŸ¨ Chat Interface

Open `test_client.html` in your browser for a beautiful chat interface with:
- âœ… Real-time streaming responses
- âœ… Query optimization display (shows rewritten query and expanded terms)
- âœ… Source document references
- âœ… Session management
- âœ… Modern, responsive UI

## ğŸ“ Example Usage with cURL

### Chat Query
```bash
curl -X POST "http://localhost:8000/chat/stream" \
  -H "Content-Type: application/json" \
  -d '{
    "message": "What are the main points about tariffs?",
    "use_query_expansion": true
  }'
```

### Upload Document
```bash
curl -X POST "http://localhost:8000/upload" \
  -F "file=@document.pdf"
```

## ğŸ”§ Features Preserved from Original Script

- âœ… **Semantic Chunking**: Uses your original merge_semantic_chunks function
- âœ… **Query Optimization**: Rewriting, expansion, and variations
- âœ… **Pinecone Integration**: Same vector database setup
- âœ… **Google Gemini LLM**: Same model and prompts
- âœ… **Document Tracking**: Uses your indexed_documents.json system
- âœ… **Ensemble Retrieval**: Vector + BM25 (when available)

## ğŸŒŸ New Features

- âœ… **REST API**: Easy integration with any frontend
- âœ… **Server-Sent Events**: Real-time streaming responses
- âœ… **Session Management**: Multi-user conversation tracking
- âœ… **Query Optimization Display**: Users see how their queries are improved
- âœ… **Interactive Docs**: Auto-generated API documentation
- âœ… **CORS Support**: Frontend integration ready

## ğŸ§ª Testing the API

### Using Python
```python
import requests

# Create session
response = requests.post("http://localhost:8000/session/new")
session_id = response.json()["session_id"]

# Ask question (non-streaming)
import json
from sseclient import SSEClient

messages = requests.post(
    "http://localhost:8000/chat/stream",
    json={
        "session_id": session_id,
        "message": "What is the document about?",
        "use_query_expansion": True
    },
    stream=True
)

for line in messages.iter_lines():
    if line:
        decoded_line = line.decode('utf-8')
        if decoded_line.startswith('data: '):
            data = json.loads(decoded_line[6:])
            print(data)
```

### Using JavaScript (Browser)
```javascript
const eventSource = new EventSource(
    `http://localhost:8000/chat/stream?message=Your question here&use_query_expansion=true`
);

eventSource.onmessage = (event) => {
    const data = JSON.parse(event.data);
    console.log(data.type, data);
};
```

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Frontend      â”‚
â”‚ (test_client.html)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ HTTP/SSE
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   FastAPI       â”‚
â”‚   (app.py)      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ Query Rewrite â”‚
â”‚ â€¢ Query Expand  â”‚
â”‚ â€¢ Retrieval     â”‚
â”‚ â€¢ LLM Generate  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â–¼         â–¼         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Pineconeâ”‚ â”‚Google  â”‚ â”‚Session â”‚
â”‚Vector  â”‚ â”‚Gemini  â”‚ â”‚Store   â”‚
â”‚  DB    â”‚ â”‚  LLM   â”‚ â”‚(Memory)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ”’ Production Considerations

Before deploying to production:

1. **Authentication**: Add JWT or API key authentication
2. **Rate Limiting**: Implement per-user rate limits
3. **Persistent Sessions**: Use Redis instead of in-memory storage
4. **Error Handling**: Add comprehensive error logging
5. **Monitoring**: Add metrics and health checks
6. **CORS**: Configure allowed origins properly
7. **HTTPS**: Use SSL certificates
8. **Environment**: Use proper secret management

## ğŸ“¦ What's Changed from Original

### Original Script (`main.py`)
- Runs once, processes documents, asks one question
- Prints everything to console
- No API interface

### New FastAPI Version (`app.py`)
- Always running, ready to serve requests
- Multiple concurrent users/sessions
- RESTful API endpoints
- Streaming responses
- Chat history tracking
- Same core logic and functions

## ğŸ› Troubleshooting

**Issue**: `uvicorn: command not found`
```bash
pip install uvicorn[standard]
```

**Issue**: CORS errors in browser
- Make sure the server is running on `http://localhost:8000`
- Check browser console for specific errors

**Issue**: "RAG system not initialized"
- Make sure Pinecone index exists with documents
- Upload at least one document using the `/upload` endpoint

**Issue**: Slow streaming
- This is normal for the word-by-word streaming effect
- Adjust the delay in `app.py` (search for `asyncio.sleep`)

## ğŸ“„ License

Same as your original project.

## ğŸ¤ Contributing

This maintains 100% compatibility with your original script while adding API functionality.

